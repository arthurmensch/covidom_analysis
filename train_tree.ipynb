{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree for PCR testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "from sklearn import tree\n",
    "import plotly.express as px\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, cross_validate, cross_val_score, KFold, StratifiedKFold\n",
    "\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms = ['tiredness',\n",
    " 'fever',\n",
    " 'shivers',\n",
    " 'cough',\n",
    " 'breathlessness',\n",
    " 'aches',\n",
    " 'chest_opression',\n",
    " 'chest_pain',\n",
    " 'diarrhea',\n",
    " 'vomiting',\n",
    " 'sensoriel', \n",
    " 'anosmia',\n",
    " 'ageusia',\n",
    " 'anorexia',\n",
    " 'rash',\n",
    " 'frostbites',\n",
    " 'conjunctivitis',\n",
    " 'other_sympt']\n",
    "\n",
    "FEATURES = ['tiredness',\n",
    " 'fever',\n",
    " 'shivers',\n",
    " 'cough',\n",
    " 'breathlessness',\n",
    " 'aches',\n",
    " 'breath_thorac',\n",
    " 'digest',\n",
    " 'sensoriel', \n",
    " 'anorexia',\n",
    " 'cutan', \n",
    " 'conjunctivitis', \n",
    "           ]\n",
    "\n",
    "FEATURE_NAMES = {'tiredness':'Tiredness',\n",
    " 'fever':'Fever',\n",
    " 'shivers':'Shivers',\n",
    " 'cough':'Cough',\n",
    " 'breathlessness':'Short of breath',\n",
    " 'aches':'Myalgia',\n",
    " 'breath_thorac':'Cardiopulmonary',\n",
    " 'digest':'Digestive symptoms',\n",
    " 'sensoriel':'Anosmia or ageusia', \n",
    " 'anorexia': 'Anorexia',\n",
    " 'cutan':'Cutaneous', \n",
    " 'conjunctivitis':'Conjunctivitis',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(main, effect_col, cols, query_filter='', \n",
    "            replace_names = {}):\n",
    "    # Filter\n",
    "    df = main.copy()\n",
    "    df = df.dropna(subset = [effect_col])\n",
    "    if query_filter!= \"\":\n",
    "        df = df.query(query_filter)\n",
    "    print(f'dropped {len(main)-len(df)} rows')\n",
    "\n",
    "    # Replace\n",
    "    for col, names in replace_names.items():\n",
    "        df[col] = df[col].replace(names)\n",
    "\n",
    "    # X, y \n",
    "    X = df[cols].values\n",
    "    y = df[effect_col].values\n",
    "    sample_weight = df['sample_weight'].values\n",
    "    \n",
    "    return X, y, sample_weight\n",
    "\n",
    "\n",
    "def gridsearch(X_train, y_train, sample_weight, hyper_params_grid) :\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        DecisionTreeClassifier(), hyper_params_grid, \n",
    "        scoring=make_scorer(average_precision_score, needs_proba=True), n_jobs=20\n",
    "    )\n",
    "    \n",
    "    print('Fitting gridsearch ... ')\n",
    "    clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    print('Done, best params : ', clf.best_params_)\n",
    "    \n",
    "    # Describe\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    print(pd.Series(means).describe())    \n",
    "    return clf.best_params_, clf.best_estimator_\n",
    "\n",
    "\n",
    "def select_variables(X_train, y_train, best_params, features, \n",
    "                    plot=False):\n",
    "    \n",
    "    # Compute permutation importance\n",
    "    cv = StratifiedShuffleSplit(n_splits=100, test_size=.5)\n",
    "    n_repeats = 10\n",
    "    mAP = []\n",
    "    fimp = np.zeros((X_train.shape[1], cv.get_n_splits(), n_repeats))\n",
    "    for i, (train, test) in enumerate(cv.split(X_train, y_train)):\n",
    "        tree = DecisionTreeClassifier(**best_params)\n",
    "        tree.fit(X_train[train], y_train[train])\n",
    "        y_pred = tree.predict_proba(X_train[test])[:,1]\n",
    "        mAP.append(average_precision_score(y_train[test], y_pred))\n",
    "        result = permutation_importance(tree, X_train[test], y_train[test], \n",
    "                                        n_repeats=n_repeats, n_jobs=5, random_state=seed, \n",
    "                                        scoring=make_scorer(average_precision_score, needs_proba=True))\n",
    "        fimp[:, i] = result.importances\n",
    "    mAP = np.mean(mAP)\n",
    "    \n",
    "    # Build df \n",
    "    ncols, nsplits, n_repeats = fimp.shape\n",
    "    df = pd.DataFrame({\n",
    "        'feature_id': np.repeat(np.arange(ncols), nsplits*n_repeats), \n",
    "        'feature':np.repeat(features, nsplits*n_repeats), \n",
    "        'fimp':fimp.reshape(-1),\n",
    "        'split':np.repeat(range(nsplits), ncols*n_repeats),\n",
    "        'permut':np.tile(range(n_repeats), ncols*nsplits),\n",
    "    })\n",
    "    df = df.groupby(['feature', 'split']).agg('mean').reset_index() # average on repeats \n",
    "    df['fimp_mean'] = df.groupby('feature')['fimp'].transform('mean')\n",
    "    \n",
    "    # Plot \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.barplot('fimp', 'feature', data=df.sort_values('fimp_mean', ascending=False), ax=ax)\n",
    "        labels = ax.get_yticklabels()\n",
    "        ax.set_yticklabels([FEATURE_NAMES[i.get_text()] for i in labels])\n",
    "        plt.xlabel('Permutation importance on test sets')\n",
    "        plt.ylabel('')\n",
    "        plt.show()\n",
    "    \n",
    "    # Select variables (cumulated sum better than chance)\n",
    "    cumsum = df.groupby('feature').agg('mean').sort_values('fimp_mean', ascending=False)['fimp_mean'].cumsum() \n",
    "    selected_variables = cumsum.index[(cumsum < (mAP-(y_train.sum()/len(y_train))))].values\n",
    "    print('Selected variables : ', selected_variables)\n",
    "                   \n",
    "    return selected_variables, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_csv('data/shuffled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import enrich_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.pipe(enrich_survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "na_labels = {'pcr_result':-1, }\n",
    "main = merged.fillna(na_labels)\n",
    "main = main.replace({'Non':0, 'Oui':1})\n",
    "main['sensoriel'] = main[['anosmia', 'ageusia']].max(axis=1)\n",
    "main['num_symp'] = main[symptoms].sum(axis=1)\n",
    "main[symptoms] = main[symptoms].fillna(0).astype('int')\n",
    "\n",
    "# Groupements \n",
    "main['chest'] = main[['chest_pain', 'chest_opression']].max(axis=1)\n",
    "main['cutan'] = main[['rash', 'frostbites']].max(axis=1)\n",
    "main['digest'] = main[['vomiting', 'diarrhea']].max(axis=1)\n",
    "main['breath_thorac'] = main[['chest', 'breathlessness']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "merged['delay_symp'] = (pd.to_datetime(merged['survey_time'])-pd.to_datetime(merged['symptom_start_time'])).dt.round('D').dt.days.astype(int)\n",
    "merged['delay_start'] = (pd.to_datetime(merged['survey_time'])-pd.to_datetime(merged['start_time'])).dt.round('D').dt.days.astype(int)\n",
    "\n",
    "df = merged.query('delay_start>=0')\n",
    "fig = plt.figure()\n",
    "sns.distplot(df['delay_start'], kde=False, bins=np.arange(100),  hist_kws={'alpha':.8})\n",
    "plt.xlim(0, 40)\n",
    "plt.xlabel('Number of days between covidom inscription and survey answer')\n",
    "plt.ylabel('Number of patients')\n",
    "plt.show()\n",
    "fig.savefig('output/charlotte_delay_covidom_answer.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.distplot(df['delay_symp'], kde=False, bins=np.arange(100), hist_kws={'alpha':.8})\n",
    "plt.xlim(0, 40)\n",
    "plt.xlabel('Number of days between first symptoms and survey answer')\n",
    "plt.ylabel('Number of patients')\n",
    "plt.show()\n",
    "fig.savefig('output/charlotte_delay_symp_answer.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "merged[['delay_symp', 'delay_start']].describe().to_csv('output/charlotte_delay_answer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['delay_symp', 'delay_start']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged.copy()\n",
    "pcr_cols = [f'eds_pcr_{rank}' for rank in np.arange(1, 11)]\n",
    "delay_cols = [f'delay_pcr_{rank}' for rank in np.arange(1, 11)]\n",
    "for col in pcr_cols:\n",
    "    df.loc[:, col] = pd.to_datetime(df[col]) #.dt.round('D')\n",
    "df[delay_cols] = df[pcr_cols].sub(df['eds_pcr_1'], axis='index')\n",
    "df = df[delay_cols].describe().T\n",
    "for col in [i for i in df.columns if i!='count']:\n",
    "    df[col] = df[col].dt.round('D')\n",
    "df.to_csv('output/charlotte_pcr_delay.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ = main.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTING = False\n",
    "ONLY_AMBULATORY = False\n",
    "\n",
    "NAME = f'{\"_correct\" if WEIGHTING else \"\"}{\"_ambulatory\" if ONLY_AMBULATORY else \"\"}'\n",
    "\n",
    "exp ={\n",
    "    'target':'pcr_result',\n",
    "    'query':'pcr_result!=-1',\n",
    "    'cols':FEATURES, \n",
    "    'label': 'PCR', \n",
    "}\n",
    "\n",
    "hyper_params_grid = {\n",
    "            'class_weight' : ['balanced', None], \n",
    "            'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 10, 20, None], \n",
    "            'min_impurity_decrease':[1e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6, 1e-7, 1e-8], \n",
    "            'min_samples_split': [5, 10, 50, 100, 500, 1000],\n",
    "            'min_samples_leaf':[50, 100, 500, 1000],\n",
    "            }\n",
    "\n",
    "N_REPEATS=50\n",
    "\n",
    "if ONLY_AMBULATORY:\n",
    "    main = main_.loc[~main['hospitalized']].copy()\n",
    "else:\n",
    "    main = main_.copy()\n",
    "\n",
    "from collections import defaultdict\n",
    "SYMPTOM_DICT = {}\n",
    "for s in ['tiredness', 'fever', 'cough', 'breathlessness', 'aches',\n",
    "          'anorexia', 'anosmia', 'ageusia', 'headache', \n",
    "#           'upper_respiratory',\n",
    "          'conjunctivitis']:\n",
    "    SYMPTOM_DICT[s] = [s]\n",
    "SYMPTOM_DICT['cutaneous'] = ['rash', 'frostbites']\n",
    "SYMPTOM_DICT['digestive'] = ['diarrhea', 'vomiting', 'abdo_pain']\n",
    "SYMPTOM_DICT['cardiopulmonary'] = ['breathlessness', 'chest_opression', 'chest_pain']\n",
    "for k, v in SYMPTOM_DICT.items():\n",
    "    main[k] = np.any(main[v], axis=1)\n",
    "SYMPTOMS = list(SYMPTOM_DICT.keys())\n",
    "\n",
    "SYMPTOMS = ['tiredness',\n",
    " 'fever',\n",
    " 'cough',\n",
    " 'breathlessness',\n",
    " 'aches',\n",
    " 'anorexia',\n",
    " 'anosmia',\n",
    " 'ageusia',\n",
    " 'headache',\n",
    " 'conjunctivitis',\n",
    " 'cutaneous',\n",
    " 'digestive',\n",
    " 'cardiopulmonary']\n",
    "\n",
    "SEX = ['male', 'female', 'undetermined']\n",
    "TOBACCO = ['smoker_current', 'no_smoker']\n",
    "COMORBIDITIES = ['no_comorbidity', 'any_comorbidity', 'respiratory', 'cardio-vascular', 'diabetes', 'obesity']\n",
    "HOSPITALIZED = ['hospitalized','non_hospitalized']\n",
    "INCLUSION_REASONS = ['samu', 'urgence']\n",
    "AGE = main['binned_age'].cat.categories.tolist()\n",
    "X_weight = main[SEX + TOBACCO + COMORBIDITIES + AGE + SYMPTOMS + HOSPITALIZED]\n",
    "\n",
    "y_weight = main['test_done'].astype(bool)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=10000)\n",
    "\n",
    "lr.fit(X_weight, y_weight)\n",
    "\n",
    "main['sample_weight'] = 1\n",
    "main['p_test'] = lr.predict_proba(X_weight)[:, 1]\n",
    "main.loc[merged['test_done'], 'sample_weight'] = 1 / main.loc[main['test_done'], 'p_test']\n",
    "main.loc[~merged['test_done'], 'sample_weight'] = 1 / (1 - main.loc[~main['test_done'], 'p_test'])\n",
    "main['sample_weight'] /= main.loc[main['test_done'], 'sample_weight'].sum() / len(main.loc[main['test_done']])\n",
    "\n",
    "if not WEIGHTING:\n",
    "    main['sample_weight'] = 1\n",
    "\n",
    "X, y, sample_weight = prepare(main, exp['target'], exp['cols'], \n",
    "               query_filter=exp['query'])\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state=seed)\n",
    "splits = list(cv.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment on one split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = splits[0]\n",
    "best_params, best_model = gridsearch(X[train], y[train], sample_weight[train], hyper_params_grid)\n",
    "tree = DecisionTreeClassifier(**best_params)\n",
    "tree.fit(X[train], y[train], sample_weight=sample_weight[train])\n",
    "\n",
    "train, test = splits[0]\n",
    "best_params = {'class_weight': 'balanced', 'max_depth': 10, 'min_impurity_decrease': 1e-07, 'min_samples_leaf': 50, 'min_samples_split': 5}\n",
    "#best_params = {'class_weight': 'balanced', 'max_depth': 10, 'min_impurity_decrease': 1e-08, 'min_samples_leaf': 100, 'min_samples_split': 5}\n",
    "tree = DecisionTreeClassifier(**best_params)\n",
    "tree.fit(X[train], y[train], sample_weight[train])\n",
    "\n",
    "# Feature importance on the test set \n",
    "sns.set_style('white')\n",
    "fig = plt.figure()\n",
    "result = permutation_importance(tree, X[test], y[test], \n",
    "                                n_repeats=N_REPEATS, n_jobs=5, random_state=seed, \n",
    "                                scoring=make_scorer(average_precision_score, needs_proba=True, sample_weight=sample_weight[test]))\n",
    "\n",
    "df = pd.DataFrame({'feats':np.repeat(exp['cols'], N_REPEATS), 'fimp':result.importances.reshape(-1)})\n",
    "df['fimp_mean'] = df.groupby('feats')['fimp'].transform('mean')\n",
    "\n",
    "# Save as csv \n",
    "df.to_csv(f'output/charlotte_fimp{NAME}.csv')\n",
    "\n",
    "# Plot \n",
    "sns.barplot('fimp', 'feats', data=df.replace(FEATURE_NAMES).sort_values('fimp_mean', ascending=False), ci='sd')\n",
    "sns.despine()\n",
    "plt.xlabel('Decrease in average precision \\n when randomly shuffling the feature')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f'output/charlotte_fimp{NAME}.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Plot performance \n",
    "fig = plt.figure()\n",
    "\n",
    "pred = tree.predict_proba(X[test])[:,1]\n",
    "precision, recall, _ = precision_recall_curve(y[test], pred, sample_weight=sample_weight[test])\n",
    "AP = average_precision_score(y[test], pred, sample_weight=sample_weight[test])\n",
    "sns.lineplot(recall, precision, label='Mean average precision: {:.2}'.format(AP))\n",
    "sns.despine()\n",
    "plt.xlabel('Recall (sensitivity)')\n",
    "plt.ylabel('Precision \\n (positive predictive value)')\n",
    "baseline = (sample_weight[test] * y[test]).sum()/sample_weight[test].sum()\n",
    "plt.axhline(baseline, label='Chance level: {:.2}'.format(baseline),\n",
    "            linestyle='--', linewidth=1, color='grey')                             \n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(f'output/charlotte_pr{NAME}.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot on train\n",
    "plt.figure(figsize=(15, 8))\n",
    "feature_names=[FEATURE_NAMES[i] for i in np.array(exp['cols'])]\n",
    "t = plot_tree(tree, feature_names=feature_names, filled=True, \n",
    "              fontsize=8, impurity=True, label=None, \n",
    "                   proportion=True, rounded=True, precision=2, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import matplotlib\n",
    "import plotly\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "\n",
    "def renumber(df_flux, nodes):\n",
    "    r = re.compile(r'\\(#[0-9]*\\)')\n",
    "    mapping = {}\n",
    "    mapping_label = {}\n",
    "    mapped_nodes = {}\n",
    "    for i, (k,v) in enumerate(nodes.items()):\n",
    "        mapping[k] = i\n",
    "        mapped_nodes[i] = copy.copy(v)\n",
    "        mapped_nodes[i]['id'] = i\n",
    "        mapped_nodes[i]['label'] = r.sub(f'(#{i:d})', v['label'])\n",
    "        mapping_label[v['label']] = mapped_nodes[i]['label']\n",
    "    df_flux = df_flux.replace(mapping)\n",
    "    nodes = mapped_nodes\n",
    "    return df_flux, nodes\n",
    "\n",
    "def build_flux(tree, cols, values=None, max_depth_plot=None, \n",
    "              use_hashtag=False, only_hashtag=False, add_stats=True):\n",
    "    t = tree.tree_\n",
    "    if values is None:\n",
    "        values = t.value.squeeze()\n",
    "    if max_depth_plot is None:\n",
    "        max_depth_plot = tree.max_depth\n",
    "    col_dico = {i:c for i,c in enumerate(cols)}\n",
    "\n",
    "    # Compute depth and is_leave\n",
    "    node_depth = np.zeros(shape=t.node_count, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=t.node_count, dtype=bool)\n",
    "    stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "        node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "        # If we have a test node\n",
    "        if (t.children_left[node_id] != t.children_right[node_id]): #and parent_depth < MAX_DEPTH_PLOT:\n",
    "            stack.append((t.children_left[node_id], parent_depth + 1))\n",
    "            stack.append((t.children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    # Build flux and nodes \n",
    "    flux = {}\n",
    "    x = {0:0, 1:.2, 2:.45, 3:.55, 4:1}\n",
    "    nodes = {i:{} for i in range(t.node_count)}\n",
    "    nodes[0]['y'] = .5\n",
    "    for i in range(t.node_count):\n",
    "        if node_depth[i] > max_depth_plot:\n",
    "            nodes[i] = None\n",
    "        else:\n",
    "            nodes[i]['x']= x[node_depth[i]] if max_depth_plot==4 else node_depth[i]/t.max_depth\n",
    "            #nodes[i]['x']= \n",
    "            nodes[i]['values']=values[i]\n",
    "#             prob = values[i][1]/values[i].sum()\n",
    "            \n",
    "#             odd = (values[i][1]/values[i][0])/(values[:,1].sum()/values[:,0].sum())\n",
    "#             nodes[i]['odd']=odd  \n",
    "#             odd = \n",
    "            name = np.array(cols)[t.feature[i]]\n",
    "            nodes[i]['name']=name\n",
    "            if use_hashtag:\n",
    "                label = '(#{}) '.format(i) \n",
    "            else:\n",
    "                label = ''\n",
    "            if not (is_leaves[i] or node_depth[i] == max_depth_plot):\n",
    "                label += '{}'.format(name)                 \n",
    "                flux[(i, t.children_left[i])] = values[t.children_left[i]].sum()\n",
    "                flux[(i, t.children_right[i])] = values[t.children_right[i]].sum()\n",
    "                step = 1/(2**node_depth[i])\n",
    "                nodes[t.children_left[i]]['y'] = nodes[i]['y'] + step/3\n",
    "                nodes[t.children_right[i]]['y'] = nodes[i]['y'] - step/3\n",
    "#             elif add_stats:\n",
    "#                 label += '{:.0%} pcr+, odds x{:.1f}'.format(prob, odd) \n",
    "            if only_hashtag : \n",
    "                label = '(#{}) '.format(i)\n",
    "            nodes[i]['label']=label\n",
    "            nodes[i]['id']=i\n",
    "            nodes[i]['final'] = is_leaves[i] or node_depth[i] == max_depth_plot\n",
    "\n",
    "    nodes = {k:v for k,v in nodes.items() if v is not None}\n",
    "\n",
    "    # Convert flux to dataframe \n",
    "    df = pd.DataFrame(flux.values(), index=flux.keys()).reset_index()\n",
    "    df.columns = ['source', 'target', 'value']\n",
    "    return renumber(df, nodes)\n",
    "\n",
    "\n",
    "def plot_tree_flow_chart(df, nodes):\n",
    "    cmap = matplotlib.cm.get_cmap('coolwarm_r')\n",
    "    norm = matplotlib.colors.DivergingNorm(vmin=0.1, vcenter=1., vmax=6)\n",
    "\n",
    "    save=True\n",
    "    fig = go.Figure(\n",
    "        data=[go.Sankey(\n",
    "            node=dict(\n",
    "                pad=10,\n",
    "                thickness=20,\n",
    "                line=dict(color=\"black\", width=0.5),\n",
    "                label = [v['name'] for k, v in nodes.iterrows()], \n",
    "                x = [v['x'] for k,v in nodes.iterrows()],\n",
    "                y = [v['y'] for k,v in nodes.iterrows()],\n",
    "                color = [f\"rgba{cmap(norm(v['odds']))}\" for k, v in nodes.iterrows()]\n",
    "            ),\n",
    "            link = dict(\n",
    "                source = [nodes.iloc[k]['id_num'] for k in df.source],\n",
    "                target = [nodes.iloc[k]['id_num'] for k in df.target], \n",
    "                value = df.value.values,\n",
    "            ),\n",
    "        )])\n",
    "    return fig\n",
    "\n",
    "def tune_nodes(nodes):\n",
    "    df = pd.DataFrame(nodes).T\n",
    "    df['Splitting criteria'] = df['name']\n",
    "    df['PCR+ patients'] = [int(np.round(i[1], 0)) for i in df['values']]\n",
    "    df['PCR- patients'] = [int(np.round(i[0], 0)) for i in df['values']]\n",
    "    df['Number of patients'] = df[['PCR+ patients', 'PCR- patients']].sum(axis=1).round(0).astype(int)\n",
    "    df['PCR+ probability'] = (df['PCR+ patients']/df['Number of patients']*100).round(0).astype(int).astype(str) + '%'\n",
    "    \n",
    "    total_patient = df['Number of patients'].iloc[0]\n",
    "    total_pcrp = df['PCR+ patients'].iloc[0]\n",
    "    y = np.zeros(total_patient)\n",
    "    y[:total_pcrp] = 1\n",
    "    \n",
    "    \n",
    "#     df['Odds ratio'] = (df['PCR+ patients']/df['PCR- patients'])/(df['PCR+ patients'].sum()/df['PCR- patients'].sum())\n",
    "#     \n",
    "#     df['Odds ratio'] = df['Odds ratio'].round(2)\n",
    "\n",
    "    \n",
    "    \n",
    "    df['id_num'] = df['id']\n",
    "    df['id'] =  '(#' + df['id'].astype(str) + ')'\n",
    "    df['Odds ratio'] = 0\n",
    "    df['Odds ratio CI'] = 0\n",
    "    for i, row in df.iterrows():\n",
    "        X = np.zeros(total_patient)\n",
    "        X[:row['PCR+ patients']] = 1\n",
    "        X[total_pcrp:total_pcrp + row['PCR- patients']] = 1\n",
    "        X = sm.add_constant(X)\n",
    "        res = sm.Logit(y, X).fit(disp=0)\n",
    "        if len(res.params) == 1:\n",
    "            odds_l, odds, odds_u = 1, 1, 1\n",
    "        else:\n",
    "            odds_l, odds, odds_u = np.exp(res.params[1] - res.bse[1]), np.exp(res.params[1]), np.exp(res.params[1] + res.bse[1])\n",
    "        \n",
    "        df.loc[i, \"odds\"] = odds\n",
    "        df.loc[i, \"Odds ratio\"] = np.round(odds, 2)\n",
    "        df.loc[i, \"Odds ratio CI\"] = f\"[{odds_l:.2f}-{odds_u:.2f}]\"\n",
    "        if row['final']:\n",
    "#             print(f'{row[\"label\"]} {row[\"PCR+ probability\"]} - odds {row[\"Odds ratio\"]:.1f}x')\n",
    "            df.loc[i, 'name'] = f'{row[\"label\"]}{row[\"PCR+ probability\"]} PCR+, odds {odds_l:.1f}-{odds_u:.1f}x'\n",
    "        else:\n",
    "            df.loc[i, 'name'] = row['label']\n",
    "    return df \n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "cmap = matplotlib.cm.get_cmap('coolwarm_r')\n",
    "# norm = matplotlib.colors.TwoSlopeNorm(vmin=0.1, vcenter=1., vmax=6)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(.3, 1.3))\n",
    "\n",
    "cb1 = matplotlib.colorbar.ColorbarBase(ax, spacing='proportional',drawedges=False, \n",
    "                                       cmap=cmap, orientation='vertical')\n",
    "sns.despine(left=True)\n",
    "\n",
    "ax.set_title('Odds ratio')\n",
    "cb1.set_ticks([0.1, 1, 6])\n",
    "#cb1.set_ticklabels([ 1, 4, 6])\n",
    "fig.savefig(f'output/charlotte_cmap.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "cols=[FEATURE_NAMES[i] for i in np.array(exp['cols'])]\n",
    "\n",
    "# Compute values for test (LONG)\n",
    "mat = tree.decision_path(X[test]).toarray()\n",
    "values = np.zeros((tree.tree_.node_count, 2))\n",
    "for sample_id, nodes in enumerate(mat):\n",
    "    target = int(y[test][sample_id])\n",
    "    values[np.nonzero(nodes), target] += sample_weight[test][sample_id]\n",
    "\n",
    "columns = ['label', 'Number of patients', 'PCR+ patients', 'PCR- patients', 'PCR+ probability', 'Odds ratio', 'Odds ratio CI']\n",
    "\n",
    "# Build flux\n",
    "df_flux, nodes = build_flux(tree, cols, values=values.copy(), max_depth_plot=4, use_hashtag=False)\n",
    "\n",
    "# Save nodes \n",
    "df_nodes = tune_nodes(nodes)\n",
    "df_nodes[columns].to_csv(f'output/charlotte_tree{NAME}.csv')\n",
    "\n",
    "# Plot \n",
    "fig = plot_tree_flow_chart(df_flux, df_nodes)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f\"Decision tree to predict PCR\",\n",
    "    width=1000,\n",
    "    height=450,\n",
    "    showlegend=True,\n",
    ")\n",
    "\n",
    "s = fig.to_html('')\n",
    "\n",
    "plotly.offline.plot(fig, filename = f'output/charlotte_tree{NAME}.html', auto_open=False)\n",
    "HTML(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot on test with hasthag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot on test with hashtag (annexe)\n",
    "\n",
    "# Build flux\n",
    "df_flux, nodes = build_flux(tree, cols, values=values.copy(), max_depth_plot=4, \n",
    "                            use_hashtag=True, add_stats=False)\n",
    "\n",
    "# Save nodes \n",
    "df_nodes = tune_nodes(nodes)\n",
    "df_nodes[columns].to_csv(f'output/charlotte_tree_hashtag{NAME}.csv')\n",
    "\n",
    "# Plot \n",
    "fig = plot_tree_flow_chart(df_flux, df_nodes)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f\"Full decision tree to predict PCR on held-out patients\",\n",
    "    width=1000,\n",
    "    height=450,\n",
    "    showlegend=True,\n",
    ")\n",
    "\n",
    "s = fig.to_html('')\n",
    "\n",
    "plotly.offline.plot(fig, filename = f'output/charlotte_tree_hashtag{NAME}.html', auto_open=False)\n",
    "HTML(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot on whole data with hashtag (annexe)\n",
    "\n",
    "# Compute values for test (LONG)\n",
    "mat = tree.decision_path(X).toarray()\n",
    "values = np.zeros((tree.tree_.node_count, 2))\n",
    "for sample_id, nodes in enumerate(mat):\n",
    "    target = int(y[sample_id])\n",
    "    values[np.nonzero(nodes), target] += sample_weight[sample_id]\n",
    "\n",
    "# Build flux\n",
    "df_flux, nodes = build_flux(tree, cols, values=values, max_depth_plot=4, add_stats=False, \n",
    "                            use_hashtag=True)\n",
    "\n",
    "# Save nodes \n",
    "df_nodes = tune_nodes(nodes)\n",
    "df_nodes[columns].to_csv(f'output/charlotte_tree_whole_data{NAME}.csv')\n",
    "\n",
    "# Plot \n",
    "fig = plot_tree_flow_chart(df_flux, df_nodes)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f\"Decision tree to predict PCR on the entire tested cohort\",\n",
    "    width=1000,\n",
    "    height=450,\n",
    "    showlegend=True,\n",
    ")\n",
    "\n",
    "s = fig.to_html('')\n",
    "\n",
    "plotly.offline.plot(fig, filename = f'output/charlotte_tree_whole_data{NAME}.html', auto_open=False)\n",
    "HTML(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fimp = []\n",
    "pr = []\n",
    "mAP = []\n",
    "PLOT=True\n",
    "for i, (train, test) in enumerate(splits):\n",
    "    best_params, best_model = gridsearch(X[train], y[train], sample_weight[train], hyper_params_grid)\n",
    "    tree = DecisionTreeClassifier(**best_params)\n",
    "    tree.fit(X[train], y[train], sample_weight=sample_weight[train])\n",
    "    \n",
    "    # Precision, recall \n",
    "    pred = tree.predict_proba(X[test])[:,1]\n",
    "    precision, recall, _ = precision_recall_curve(y[test], pred, sample_weight=sample_weight[test])\n",
    "    pr.append((precision, recall))\n",
    "    AP = average_precision_score(y[test], pred, sample_weight=sample_weight[test]) \n",
    "    mAP.append(AP)\n",
    "\n",
    "    # Permutation importance \n",
    "    result = permutation_importance(tree, X[test], y[test],\n",
    "                                n_repeats=N_REPEATS, n_jobs=5, random_state=seed, \n",
    "                                scoring=make_scorer(average_precision_score, needs_proba=True, sample_weight=sample_weight[test]))\n",
    "    fimp_df = pd.DataFrame({'fimp':result.importances.reshape(-1), \n",
    "                            'feats':np.repeat(exp['cols'], N_REPEATS), \n",
    "                           })\n",
    "    fimp_df['cross_val_split'] = i\n",
    "    all_fimp.append(fimp_df)\n",
    "    \n",
    "    # Plot tree \n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        feature_names=[FEATURE_NAMES[i] for i in np.array(exp['cols'])]\n",
    "        t = plot_tree(tree, feature_names=feature_names, filled=True, \n",
    "                      fontsize=8, impurity=True, label=None, \n",
    "                           proportion=True, rounded=True, precision=2, max_depth=4)\n",
    "\n",
    "all_fimp = pd.concat(all_fimp, axis=0)\n",
    "\n",
    "# Save as csv \n",
    "all_fimp.to_csv('output/charlotte_fimp_cross_val.csv')\n",
    "np.save('output/charlotte_pr_cross_val.npy', pr)\n",
    "\n",
    "# Plot RF performance on test sets \n",
    "sns.set_style()\n",
    "fig = plt.figure()\n",
    "for i, (precision, recall) in enumerate(pr):\n",
    "    sns.lineplot(recall, precision, label=str(i))\n",
    "sns.despine()\n",
    "plt.xlabel('Recall (sensitivity)')\n",
    "plt.ylabel('Precision \\n (positive predictive value)')\n",
    "plt.axhline((sample_weight[test] * y[test]).sum() / sample_weight[test].sum(), label='Chance level', linestyle='--', linewidth=1, color='grey')\n",
    "plt.title('Precision recall curve on test set \\n for 5 cross validation splits')\n",
    "plt.legend(title='Cross validation splits', loc='upper right')\n",
    "plt.show()\n",
    "fig.savefig(f'output/charlotte_pr_cross_val{NAME}.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Plot variables \n",
    "all_fimp = pd.read_csv('output/charlotte_fimp_cross_val.csv')\n",
    "fig, ax = plt.subplots()\n",
    "all_fimp['feature_mean'] = all_fimp.groupby('feats')['fimp'].transform('mean')\n",
    "sns.barplot('fimp', 'feats', data=all_fimp.sort_values('feature_mean', ascending=False), \n",
    "            ax=ax, hue='cross_val_split', ci='sd')\n",
    "sns.despine()\n",
    "labels = ax.get_yticklabels()\n",
    "ax.set_yticklabels([FEATURE_NAMES[i.get_text()] for i in labels])\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Decrease in average precision \\n when randomly shuffling the feature')\n",
    "plt.title('Feature permutation importance on test sets \\n for 5 cross validation splits')\n",
    "plt.legend(title='Cross validation splits')\n",
    "plt.show()\n",
    "fig.savefig(f'output/charlotte_fimp_cross_val{NAME}.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
